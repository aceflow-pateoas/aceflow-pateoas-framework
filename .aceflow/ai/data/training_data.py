#!/usr/bin/env python3
"""
AceFlow v2.0 AIè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
ä¸ºä»»åŠ¡åˆ†ç±»å’Œæµç¨‹æ¨èæ¨¡å‹æä¾›è®­ç»ƒæ•°æ®
"""

import json
import random
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Any
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum

# å¯¼å…¥å†³ç­–å¼•æ“çš„æšä¸¾ç±»å‹
import sys
sys.path.append(str(Path(__file__).parent.parent))

from engines.decision_engine import TaskType, ProjectComplexity, TaskContext, ProjectContext

class TrainingDataGenerator:
    """è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.task_keywords = {
            TaskType.FEATURE_DEVELOPMENT: [
                "add new feature", "implement functionality", "create component",
                "build module", "develop interface", "add capability", "æ–°åŠŸèƒ½",
                "å®ç°åŠŸèƒ½", "å¼€å‘ç»„ä»¶", "åˆ›å»ºæ¨¡å—", "æ„å»ºæ¥å£"
            ],
            TaskType.BUG_FIX: [
                "fix bug", "resolve issue", "correct error", "patch problem",
                "debug issue", "solve crash", "ä¿®å¤bug", "è§£å†³é—®é¢˜", "ä¿®æ­£é”™è¯¯",
                "è°ƒè¯•é—®é¢˜", "è§£å†³å´©æºƒ"
            ],
            TaskType.REFACTORING: [
                "refactor code", "improve structure", "optimize performance",
                "clean up code", "restructure", "é‡æ„ä»£ç ", "ä¼˜åŒ–ç»“æ„",
                "æ€§èƒ½ä¼˜åŒ–", "ä»£ç æ•´ç†", "é‡æ–°æ¶æ„"
            ],
            TaskType.TESTING: [
                "write tests", "add unit tests", "create test cases",
                "test functionality", "write integration tests", "ç¼–å†™æµ‹è¯•",
                "å•å…ƒæµ‹è¯•", "é›†æˆæµ‹è¯•", "æµ‹è¯•ç”¨ä¾‹", "åŠŸèƒ½æµ‹è¯•"
            ],
            TaskType.DOCUMENTATION: [
                "update documentation", "write docs", "create guide",
                "document API", "write README", "æ›´æ–°æ–‡æ¡£", "ç¼–å†™æ–‡æ¡£",
                "åˆ›å»ºæŒ‡å—", "APIæ–‡æ¡£", "è¯´æ˜æ–‡æ¡£"
            ],
            TaskType.RESEARCH: [
                "research technology", "investigate solution", "explore options",
                "study framework", "analyze tools", "æŠ€æœ¯è°ƒç ”", "æ–¹æ¡ˆç ”ç©¶",
                "æŠ€æœ¯æ¢ç´¢", "æ¡†æ¶åˆ†æ", "å·¥å…·ç ”ç©¶"
            ],
            TaskType.ARCHITECTURE: [
                "design system", "plan architecture", "create blueprint",
                "define structure", "design patterns", "ç³»ç»Ÿè®¾è®¡", "æ¶æ„è®¾è®¡",
                "æ¶æ„è§„åˆ’", "è®¾è®¡æ¨¡å¼", "ç³»ç»Ÿæ¶æ„"
            ],
            TaskType.DEPLOYMENT: [
                "deploy application", "release version", "publish build",
                "setup environment", "configure deployment", "éƒ¨ç½²åº”ç”¨",
                "å‘å¸ƒç‰ˆæœ¬", "ç¯å¢ƒé…ç½®", "éƒ¨ç½²é…ç½®", "ä¸Šçº¿éƒ¨ç½²"
            ]
        }
        
        self.project_types = ["web", "mobile", "api", "desktop", "game", "ai", "blockchain"]
        self.tech_stacks = [
            ["Python", "Django", "PostgreSQL"],
            ["JavaScript", "React", "Node.js"],
            ["Java", "Spring", "MySQL"],
            ["C#", ".NET", "SQL Server"],
            ["Go", "Gin", "Redis"],
            ["TypeScript", "Vue.js", "MongoDB"],
            ["Swift", "iOS", "CoreData"],
            ["Kotlin", "Android", "SQLite"]
        ]
        
        self.priorities = ["high", "medium", "low"]
        self.complexities = ["high", "medium", "low"]
        self.impacts = ["high", "medium", "low"]
        
    def generate_task_description(self, task_type: TaskType) -> str:
        """ç”Ÿæˆä»»åŠ¡æè¿°"""
        keywords = self.task_keywords[task_type]
        base_keyword = random.choice(keywords)
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        contexts = [
            "for user dashboard",
            "in payment module", 
            "for mobile app",
            "in admin panel",
            "for API endpoint",
            "in authentication system",
            "for data visualization",
            "åœ¨ç”¨æˆ·ç®¡ç†æ¨¡å—",
            "åœ¨æ”¯ä»˜ç³»ç»Ÿä¸­",
            "åœ¨ç§»åŠ¨ç«¯åº”ç”¨",
            "åœ¨åå°ç®¡ç†"
        ]
        
        context = random.choice(contexts)
        
        # ç”Ÿæˆå®Œæ•´æè¿°
        if random.random() < 0.5:
            return f"{base_keyword} {context}"
        else:
            return f"{base_keyword} to improve {context}"
    
    def generate_task_context(self, task_type: TaskType) -> TaskContext:
        """ç”Ÿæˆä»»åŠ¡ä¸Šä¸‹æ–‡"""
        description = self.generate_task_description(task_type)
        priority = random.choice(self.priorities)
        complexity = random.choice(self.complexities)
        impact = random.choice(self.impacts)
        
        # ç”Ÿæˆä¾èµ–å…³ç³»
        dependencies = []
        if random.random() < 0.3:  # 30%æ¦‚ç‡æœ‰ä¾èµ–
            dep_count = random.randint(1, 3)
            for i in range(dep_count):
                dependencies.append(f"dependency_{i+1}")
        
        # ç”Ÿæˆä¼°ç®—æ—¶é—´
        effort_options = ["1-2 hours", "half day", "1-2 days", "3-5 days", "1 week", "2 weeks"]
        effort = random.choice(effort_options)
        
        return TaskContext(
            description=description,
            priority=priority,
            estimated_effort=effort,
            dependencies=dependencies,
            technical_complexity=complexity,
            user_impact=impact
        )
    
    def generate_project_context(self, preferred_flow: str = None) -> ProjectContext:
        """ç”Ÿæˆé¡¹ç›®ä¸Šä¸‹æ–‡"""
        project_type = random.choice(self.project_types)
        tech_stack = random.choice(self.tech_stacks)
        
        # æ ¹æ®æµç¨‹ç±»å‹è°ƒæ•´é¡¹ç›®ç‰¹å¾
        if preferred_flow == "minimal":
            team_size = random.randint(1, 3)
            duration = random.choice(["1 week", "2 weeks", "1 month"])
            risk_count = random.randint(0, 2)
        elif preferred_flow == "complete":
            team_size = random.randint(8, 15)
            duration = random.choice(["3 months", "6 months", "1 year"])
            risk_count = random.randint(3, 6)
        else:  # standard
            team_size = random.randint(3, 8)
            duration = random.choice(["1 month", "2 months", "3 months"])
            risk_count = random.randint(1, 3)
        
        # ç”Ÿæˆé£é™©å› ç´ 
        risk_factors = []
        risk_options = [
            "tight deadline", "new technology", "complex requirements",
            "limited resources", "high complexity", "uncertain scope",
            "integration challenges", "performance requirements"
        ]
        
        for _ in range(risk_count):
            risk_factors.append(random.choice(risk_options))
        
        # ç”Ÿæˆå…¶ä»–å±æ€§
        stages = ["planning", "development", "testing", "deployment"]
        current_stage = random.choice(stages)
        completion = random.uniform(0.1, 0.9)
        velocity = random.uniform(0.6, 1.2)
        
        return ProjectContext(
            name=f"{project_type}-project-{random.randint(1000, 9999)}",
            team_size=team_size,
            duration_estimate=duration,
            technology_stack=tech_stack,
            project_type=project_type,
            current_stage=current_stage,
            completion_percentage=completion,
            historical_velocity=velocity,
            risk_factors=risk_factors
        )
    
    def generate_task_classification_data(self, samples_per_type: int = 50) -> List[Tuple[str, TaskType]]:
        """ç”Ÿæˆä»»åŠ¡åˆ†ç±»è®­ç»ƒæ•°æ®"""
        training_data = []
        
        for task_type in TaskType:
            for _ in range(samples_per_type):
                task_context = self.generate_task_context(task_type)
                training_data.append((task_context.description, task_type))
        
        # æ‰“ä¹±æ•°æ®é¡ºåº
        random.shuffle(training_data)
        return training_data
    
    def generate_flow_recommendation_data(self, samples_per_flow: int = 100) -> List[Tuple[Dict[str, Any], str]]:
        """ç”Ÿæˆæµç¨‹æ¨èè®­ç»ƒæ•°æ®"""
        training_data = []
        flows = ["minimal", "standard", "complete"]
        
        for flow in flows:
            for _ in range(samples_per_flow):
                # ç”Ÿæˆé€‚åˆè¯¥æµç¨‹çš„é¡¹ç›®ä¸Šä¸‹æ–‡
                project_context = self.generate_project_context(flow)
                features = project_context.to_features()
                
                training_data.append((features, flow))
        
        # æ‰“ä¹±æ•°æ®é¡ºåº
        random.shuffle(training_data)
        return training_data
    
    def generate_progress_prediction_data(self, samples: int = 500) -> List[Tuple[Dict[str, Any], int]]:
        """ç”Ÿæˆè¿›åº¦é¢„æµ‹è®­ç»ƒæ•°æ®"""
        training_data = []
        
        for _ in range(samples):
            task_type = random.choice(list(TaskType))
            task_context = self.generate_task_context(task_type)
            project_context = self.generate_project_context()
            
            # åŸºäºè§„åˆ™ç”Ÿæˆ"çœŸå®"æŒç»­æ—¶é—´
            base_hours = {
                TaskType.BUG_FIX: 4,
                TaskType.FEATURE_DEVELOPMENT: 16,
                TaskType.REFACTORING: 8,
                TaskType.TESTING: 6,
                TaskType.DOCUMENTATION: 4,
                TaskType.RESEARCH: 12,
                TaskType.ARCHITECTURE: 24,
                TaskType.DEPLOYMENT: 8
            }
            
            hours = base_hours.get(task_type, 8)
            
            # æ·»åŠ éšæœºå˜åŒ–å’Œé¡¹ç›®å› ç´ å½±å“
            if task_context.technical_complexity == "high":
                hours *= random.uniform(1.3, 1.8)
            elif task_context.technical_complexity == "low":
                hours *= random.uniform(0.7, 0.9)
            
            if project_context.team_size > 5:
                hours *= random.uniform(1.1, 1.4)
            elif project_context.team_size == 1:
                hours *= random.uniform(0.8, 1.0)
            
            if len(project_context.risk_factors) > 2:
                hours *= random.uniform(1.2, 1.6)
            
            # æ·»åŠ éšæœºå™ªå£°
            hours *= random.uniform(0.8, 1.3)
            
            # åˆå¹¶ç‰¹å¾
            features = {
                **task_context.to_features(),
                **project_context.to_features(),
                'task_type_feature': task_type.value
            }
            
            training_data.append((features, int(hours)))
        
        return training_data
    
    def save_training_data(self, output_dir: Path):
        """ä¿å­˜è®­ç»ƒæ•°æ®åˆ°æ–‡ä»¶"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆä»»åŠ¡åˆ†ç±»æ•°æ®
        print("ğŸ”„ ç”Ÿæˆä»»åŠ¡åˆ†ç±»è®­ç»ƒæ•°æ®...")
        task_data = self.generate_task_classification_data(samples_per_type=100)
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        task_data_serializable = [
            (description, task_type.value) for description, task_type in task_data
        ]
        
        with open(output_dir / "task_classification_data.json", 'w', encoding='utf-8') as f:
            json.dump(task_data_serializable, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ä»»åŠ¡åˆ†ç±»æ•°æ®å·²ä¿å­˜: {len(task_data)} æ¡è®°å½•")
        
        # ç”Ÿæˆæµç¨‹æ¨èæ•°æ®
        print("ğŸ”„ ç”Ÿæˆæµç¨‹æ¨èè®­ç»ƒæ•°æ®...")
        flow_data = self.generate_flow_recommendation_data(samples_per_flow=150)
        
        with open(output_dir / "flow_recommendation_data.json", 'w', encoding='utf-8') as f:
            json.dump(flow_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æµç¨‹æ¨èæ•°æ®å·²ä¿å­˜: {len(flow_data)} æ¡è®°å½•")
        
        # ç”Ÿæˆè¿›åº¦é¢„æµ‹æ•°æ®
        print("ğŸ”„ ç”Ÿæˆè¿›åº¦é¢„æµ‹è®­ç»ƒæ•°æ®...")
        progress_data = self.generate_progress_prediction_data(samples=800)
        
        with open(output_dir / "progress_prediction_data.json", 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è¿›åº¦é¢„æµ‹æ•°æ®å·²ä¿å­˜: {len(progress_data)} æ¡è®°å½•")
        
        # ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "generation_time": datetime.now().isoformat(),
            "total_samples": len(task_data) + len(flow_data) + len(progress_data),
            "task_classification_samples": len(task_data),
            "flow_recommendation_samples": len(flow_data),
            "progress_prediction_samples": len(progress_data),
            "task_types": [t.value for t in TaskType],
            "flow_types": ["minimal", "standard", "complete"],
            "feature_dimensions": {
                "task_features": len(self.generate_task_context(TaskType.FEATURE_DEVELOPMENT).to_features()),
                "project_features": len(self.generate_project_context().to_features())
            }
        }
        
        with open(output_dir / "dataset_stats.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜")
        print(f"ğŸ“ æ‰€æœ‰è®­ç»ƒæ•°æ®ä¿å­˜åœ¨: {output_dir}")


class RealWorldDataCollector:
    """çœŸå®é¡¹ç›®æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, aceflow_dir: Path):
        self.aceflow_dir = aceflow_dir
        self.data_dir = aceflow_dir / "ai" / "data"
        self.real_data_file = self.data_dir / "real_world_data.jsonl"
    
    def collect_project_data(self) -> Dict[str, Any]:
        """æ”¶é›†å½“å‰é¡¹ç›®çš„çœŸå®æ•°æ®"""
        try:
            # è¯»å–é¡¹ç›®çŠ¶æ€
            state_file = self.aceflow_dir / "state" / "project_state.json"
            if state_file.exists():
                with open(state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
            else:
                state = {}
            
            # è¯»å–é…ç½®
            config_file = self.aceflow_dir / "config.yaml"
            if config_file.exists():
                import yaml
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
            else:
                config = {}
            
            # æ”¶é›†é¡¹ç›®ç‰¹å¾
            project_data = {
                "timestamp": datetime.now().isoformat(),
                "project_id": state.get("project_id", "unknown"),
                "flow_mode": state.get("flow_mode", "unknown"),
                "current_stage": state.get("current_stage", "unknown"),
                "progress": state.get("progress", {}),
                "team_size": config.get("project", {}).get("team_size", "unknown"),
                "project_type": config.get("project", {}).get("project_type", "unknown"),
                "agile_framework": config.get("agile", {}).get("framework", "unknown"),
                "stage_history": state.get("stage_states", {})
            }
            
            return project_data
            
        except Exception as e:
            print(f"âš ï¸ æ”¶é›†é¡¹ç›®æ•°æ®æ—¶å‡ºé”™: {e}")
            return {}
    
    def save_real_data(self, data: Dict[str, Any]):
        """ä¿å­˜çœŸå®æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            self.data_dir.mkdir(parents=True, exist_ok=True)
            
            with open(self.real_data_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
            
            print(f"âœ… çœŸå®æ•°æ®å·²ä¿å­˜: {self.real_data_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜çœŸå®æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def get_collected_data(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ”¶é›†çš„çœŸå®æ•°æ®"""
        data = []
        
        if self.real_data_file.exists():
            try:
                with open(self.real_data_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data.append(json.loads(line.strip()))
            except Exception as e:
                print(f"âš ï¸ è¯»å–çœŸå®æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return data


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– AceFlow v2.0 AIè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
    random.seed(42)
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    current_dir = Path(__file__).parent
    output_dir = current_dir / "training_datasets"
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    generator = TrainingDataGenerator()
    generator.save_training_data(output_dir)
    
    # æ”¶é›†çœŸå®é¡¹ç›®æ•°æ®
    aceflow_dir = current_dir.parent.parent
    collector = RealWorldDataCollector(aceflow_dir)
    
    real_data = collector.collect_project_data()
    if real_data:
        collector.save_real_data(real_data)
    
    print("\nğŸ‰ è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ!")
    print("ğŸ“‹ æ•°æ®é›†åŒ…å«:")
    print("   - ä»»åŠ¡åˆ†ç±»æ•°æ®: 800æ¡è®°å½•")
    print("   - æµç¨‹æ¨èæ•°æ®: 450æ¡è®°å½•") 
    print("   - è¿›åº¦é¢„æµ‹æ•°æ®: 800æ¡è®°å½•")
    print("   - çœŸå®é¡¹ç›®æ•°æ®: æŒç»­æ”¶é›†ä¸­")
    print(f"\nğŸ’¾ æ•°æ®ä¿å­˜ä½ç½®: {output_dir}")


if __name__ == "__main__":
    main()